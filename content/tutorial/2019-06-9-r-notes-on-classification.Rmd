---
title: "R Notes on Classification, Clustering, etc."
author: "Luisa M. Mimmi"
date: "Last run: `r format(Sys.time(), '%B %e, %Y')`"
draft: false
output:
  blogdown::html_page:
    toc: true
    #toc_float: true # NOT AVAIL FOR HUGO ACADEMIC  https://github.com/rstudio/blogdown/issues/58
    #smart: true
image:
  caption: ''
  focal_point: ''
slug: r-notes-on-classification
tags:
  - clustering
  - classification
  - K-nn
  - K-mean
categories:
  - Rtutorial
summary: "Recap of Classification - UNFINISHED" 
---



## Set up 
```{r setup, eval= TRUE, echo = FALSE, message=FALSE, warning=FALSE}
# I am  executing bc it for the blog post
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	results = "hide",
	tidy = TRUE,
	fig.show = "asis",
	fig.align = "center")

# packg
if (!require("pacman")) install.packages("pacman")
p_load(PerformanceAnalytics,
		 ggcorrplot,
		 ggpubr,
		 GGally,
		 kableExtra,
		 pander, 
		 gmodels, 
		 caret)


# POV is where .toml is? 
load(here::here("content","tutorial", "AC_ppl_fakeID.Rdata"), verbose = F)

```

 

# CLASSIFICATION 

Assign an observation to the "class" where it belongs: 


NOTES: 
- (*) _typically not useful for understanding the nature of teh relationship between the feature and class outcome (aka the sign), but can be very effective as "black box"prediction engines" _
- (*) _closely depends on available variables _


TYPES

+ **SUPERVISED**: I DO have the actual "label" 
	* criterion become _accuracy_ of the prediction

+ **UNSUPERVISED**: I don't have the actual "label" 
	* criterion (to shape groups) become _similarity_

Examples:

+ spam filter
+ genetic testing
+ fraud detection
+ psicological diagnosis


ALGORITHMs:

+ K-nn ( Find the number of cases in the multidimensional space that are closest to the new one)
+ k-means
+ Decision trees
+ Random forest
+ Naive Bayes (start prior probability (eg of having cancer) then  naive Bayes adjusts the probability - posterior probability-  with each new piece of information, )
+ SVM
+ ANN (are multiple layers of equations and weights to model nonlinear outcomes)
+ Logistic regression

	

### k-Nearest-Neighbors (kNN)	CLASSIFICATION

NOTES: 
- (*) _"Closest" usually is defined by Euclidean distance (OF for quantitative features) in feature space_ 
- (*) _You typically first standardize each feature  to have mean =0 and variance =1 in the training sample_
- (*) _The Nearest-Neighbors technique can also be used in regressoin and works reasonably well for low-dimensional problems. Instead, with high-dimensional features, the bias-variance tradeoff does not work in regr as it does in classificationo_



#### 1/2) Example Credit Default data - using `class::knn`

> colnames(df)
[1] "ID"        "LIMIT_BAL" "SEX"       "EDUCATION" "MARRIAGE"  "AGE"       "PAY_0"     "PAY_2"     "PAY_3"     "PAY_4"    
[11] "PAY_5"     "PAY_6"     "BILL_AMT1" "BILL_AMT2" "BILL_AMT3" "BILL_AMT4" "BILL_AMT5" "BILL_AMT6" "PAY_AMT1"  "PAY_AMT2" 
[21] "PAY_AMT3"  "PAY_AMT4"  "PAY_AMT5"  "PAY_AMT6"  "DEFAULT" 


```{r knn_class}
# DM_04_03.R

# LOAD PACKAGES ############################################
pacman::p_load("class")  # class has knn function

# LOAD DATA ################################################
# Read CSV 
df_big <- read.csv(here::here("content","tutorial", "data", "ccdefault.csv"), header = T)
# I want a smaller dataset 
df<- df_big[sample(nrow(df_big),replace=F,size=0.05*nrow(df_big)),]
 
# NORMALIZE DATA ###########################################
# If ranges for variables are very different, then it's a
# good idea to normalize the variables, which puts them in
# similar ranges. Use custom function for now.

# Define function for normalizing data
normalize <- function(x) {
	num <- x - min(x)
	denom <- max(x) - min(x)
	return (num/denom)
}

# Apply function to data frame (but not index or outcome)
dfn <- as.data.frame(lapply(df[, 2:24], normalize))
head(dfn)

# Put outcome variable back on and rename
dfn <- cbind(dfn, df[, 25])
names(dfn)[24] <- "DEFAULT"

# Check data
colnames(dfn)
head(dfn)

# SPLIT DATA ###############################################

# Split data into training set (2/3) and testing set (1/3)
set.seed(2786)  # Random seed
dfn.split <- sample(2, 
						  nrow(dfn), 
                    replace = TRUE,# assign a 1 or a 2 to a certain row and then reset the vector of 2 to its original state
                    prob = c(2/3, 1/3))

# Create training and testing datasets without outcome
# labels. Use just the first 23 variables.
dfn.train <- dfn[dfn.split == 1, 1:23]
dfn.test  <- dfn[dfn.split == 2, 1:23]

# Create outcome labels
dfn.train.labels <- dfn[dfn.split == 1, 24]
dfn.test.labels  <- dfn[dfn.split == 2, 24]

# BUILD AND TEST knn CLASSIFIER ################################
# Build classifier for test data.
# k = number of neighbors to compare; odd n avoids ties.
# Try with several values of k and check accuracy on following table.
dfn.pred <- class::knn(train = dfn.train,
                test = dfn.test, 
                cl = dfn.train.labels,  # true class
                k = 9)                  # you set k = n neighbors

dfn.pred 
# -> is a factor vector with the predicted classes for each row of the test data 



# ================ Evaluate  predicted outcome to observed outcome
table(dfn.pred, dfn.test.labels)

# ================ Evaluate  predicted outcome 
# Put `dfn.test.labels` in a data frame
dfn.test.labels_df <- data.frame(dfn.test.labels)

# Merge `iris_pred` and `dfn.test.labels_df` 
merge <- data.frame(dfn.pred, dfn.test.labels_df)

# Specify column names for `merge`
names(merge) <- c("Predicted Species", "Observed Species")

# Visually Inspect `merge`  and get an idea of how well it predicted 
head(merge)

# ================ FURTHER Evaluate predicted outcome `gmodels`
# make a cross tabulation or a contingency table
gmodels::CrossTable(x = dfn.test.labels, y = dfn.pred, 
						  prop.chisq=FALSE,
						  format = "SPSS")

# argument `prop.chisq` =  chi-square statistic is the sum of the contributions from each of the individual cells and is used to decide whether the difference between the observed and the expected values is significant.

```

 
 	
#### 2a/2) Example Credit Default data - using `caret`
	
> 	the `caret` package picks the optimal number of neighbors (k) for you
	
```{r knn_careta}
# LOAD PACKAGES ############################################
# caret

# LOAD DATA ################################################
# Read CSV 
# df_big <- read.csv(here::here("content","tutorial", "data", "ccdefault.csv"), header = T)
# I want a smaller dataset 
# df<- df_big[sample(nrow(df_big),replace=F,size=0.05*nrow(df_big)),]

# NORMALIZE  ################################################
# Apply function to data frame (but not index or outcome)
# dfn <- as.data.frame(lapply(df[, 2:24], normalize))
# head(dfn)

# Put outcome variable back on and rename
dfn <- cbind(dfn, df[, 25])
names(dfn)[24] <- "DEFAULT"
 
# LABEL must be a factor 
 dfn$DEFAULT <- as.factor (dfn$DEFAULT)

 
# Create index to split based on labels  
index <- caret::createDataPartition(dfn$DEFAULT, p=0.75, list=FALSE)

# Subset training set with index
dfn.training <- dfn[index,]

# Subset test set with index
dfn.test <- dfn[-index,]

# ========= TRAIN MODEL ################################################
# Overview of algos supported by caret
# names(getModelInfo())

# Train a model
model_knn <- caret::train(dfn.training[, 1:23], dfn.training[, 24], method='knn')

# How many k did caret  use? 
model_knn # 9
model_knn$bestTune
#Use plots to see optimal number of clusters:
#Plotting yields Number of Neighbours Vs accuracy (based on repeated cross validation)
plot(model_knn)


# PREDICT   ################################################
# Predict the labels of the test set
predictions <- predict.train(object= model_knn, dfn.test[ ,1:23], type="raw")

# Evaluate the predictions
table(predictions)

# Confusion matrix 
confusionMatrix(predictions, dfn.test[,24])
```


#### 2b/2) Example Credit Default data - using `caret` + PRE-PROCESSING

Use data without normalization and preprocess with caret 

>>> Some knitr error here 

```{r knn_caret_b, eval=FALSE, include=FALSE}
#   
# # LOAD DATA ################################################
# # Read CSV 
# #df_big <- read.csv(here::here("content","tutorial", "data", "ccdefault.csv"), header = T)
# # I want a smaller dataset 
# #df<- df_big[sample(nrow(df_big),replace=F,size=0.05*nrow(df_big)),]
# 
#  
# # LABEL must be a factor 
# # df$DEFAULT <- as.factor (df$DEFAULT)
# 
#  
# # Create index to split based on labels  
# index <- caret::createDataPartition(df$DEFAULT, p=0.75, list=FALSE)
# 
# # Subset training set with index
# df.training <- df[index,]
# 
# # Subset test set with index
# df.test <- df[-index,]
# 
# # ========= TRAIN MODEL W/ PREPROCESSING you can try to perform the same test as before, to examine the effect of preprocessing, such as scaling and centering, on your model.     
# # Train the model with preprocessing
# model_knn2 <- train(df.training[, 1:23], df.training[, 24], method='knn', 
# 						  preProcess=c("center", "scale") #,
# 						  # tuneLength = 20
# 						  )
# 
# model_knn2 # shows me options of K
# 
# # How many k did caret  use?   9 !!!
# model_knn2$bestTune
# 
# 
# #Use plots to see optimal number of clusters:
# #Plotting yields Number of Neighbours Vs accuracy (based on repeated cross validation)
# plot(model_knn2)
# 
# # Predict values
# predictions2 <- predict.train(object=model_knn2, df.test[,1:23], 
# 									 type="raw" # either "raw" or "prob", for the number/class predictions or class probabilities 
# 									 ) 
# 
# # Confusion matrix
# confusionMatrix(predictions2, df.test[,24])
```


	
### EXE Decision trees

> add from NYEE example
https://opendatascience.com/the-complete-guide-to-decision-trees-part-1/





### EXE Naive Bayes // Bayesian Classifier 

Builds upon Bayes Rule

<!-- Using markdown way -->
![Bayes Rule](/tutorial/tut_img/Bayes.png)


(*) The naiveBayes function takes in numeric or factor variables in a data frame or a numeric matrix. It’s important to note that single vectors will not work for the input data but will work for the dependent variable (Y).

    * Factor variables and Character variables are accepted.
    * Character variables are coerced into Factors.
    * Numeric variables will be placed on a normal distribution.
    * Then the numeric variable will be converted into a probability on that distribution.

(*) http://www.learnbymarketing.com/tutorials/naive-bayes-in-r/
```{r}
## ------------------------------------------------------------------------
SpamData<-read.csv(here::here("content","tutorial", "data", "SpamData2.csv"))

## ------------------------------------------------------------------------
head(SpamData)
mean(SpamData$Spam)

## ------------------------------------------------------------------------
SpamEmails<-subset(SpamData,Spam==1)
mean(SpamEmails$exclamation_point==1)

# Challenge: prove that the maximum likelihood estiamte of θj,y corresponds to the proportion of emails in category y that contain the feature j.
# ================ Let’s now run our Bayesian classifier. We will a package already implementing a Bayesian classifier in R that will repeat what we just did for all the features.

## ------------------------------------------------------------------------
#install.packages("e1071")
library(e1071)

## ------------------------------------------------------------------------
# We will also split our sample into training and testing data:

#install.packages("caTools")
library(caTools)
SpamData$sample<-sample.split(SpamData$Spam,SplitRatio=0.8)
train=subset(SpamData, SpamData$sample==TRUE)
test=subset(SpamData, SpamData$sample==FALSE)

train<-train[,1:55]
test<-test[,1:55]


## ------------------------------------------------------------------------
#  We call our naiveBayes function with to estimate the model and we evaluate its performance on the testing dataset
# Create the model on train 
# model 	<- naiveBayes(Class ~ ., 			data = HouseVotes84)
nB_model <- naiveBayes(as.factor(Spam) ~.,  data=train)
 
# Predict (using the model) on test 
pred_vect <- predict(nB_model, test[,1:54], type="class")
table(pred_vect, as.factor(test$Spam) ,dnn=c("Prediction","Actual"))

pred_T <- table(pred_vect, as.factor(test$Spam) )
pred_T
colnames(pred_T)<-c("predicted non-spam","predicted spam")
rownames(pred_T)<-c("true non-spam", "true spam")
# make %
pred_T <- pred_T/sum(pred_T)
pred_T

## ------------------------------------------------------------------------
#Precission:
pred_T[2,2]/(pred_T[2,2]+pred_T[1,2])

#Recall
pred_T[2,2]/(pred_T[2,2]+pred_T[2,1])

##Accuracy
pred_T[2,2]+pred_T[1,1]



```


	

*** 

# CLUSTER ANALISYS

Clustering is "pragmatic grouping" done for a reason that make sense to me. 
It is considered `Exploratory Data Mining` bc it helps tease out relationships in large datasets 
Examples:
+ marketing segmentation  - similar customers
+ medicine - simiar patients
+ music - genre on playlist that I want together

ALGORITHMs: 
+ by **distance** (only convex clusters)
	- euclidean distance
	- connectivity models
	- hierarchical diagrams
	- joining or splitting

+ by **distance from centroid**  (only convex clusters similar size, you pick k)
	- (mean for every group)
	- k-means clustering

+ by **density**
	- connected dense regions in k-dimensions
 	+ model nonconvex clusters
	+ it is possible to sort of ignore outliers 
	- hard to describe (by parametric distrib function)
	

+ by **distribution model**
	- clusters as stat distributions 
	- (ex. multivariate normal)
	- prone to overfitting
	- good to see correlations b/w attributes in data 
	- limited to convex clusters 


## Three major kinds of clustering:
   1. Hierarchical: Start separate and combine
   2. Split into set number of clusters (e.g., kmeans)
   3. Dividing: Start with a single group and split



### EXE 1/2 HIERARCHICAL CLUST 
+ by **distance** (only convex clusters)

```{r}
# DM_03_03.R

# LOAD DATA ################################################
# POV is where .toml is? 
states <- read.csv(here::here("content","tutorial", "data","ClusterData.csv"), header = T)

colnames(states)

# Save numerical data only
st <- states[, 3:27]
row.names(st) <- states[, 2]
colnames(st)

# Sports search data only
sports <- st[, 8:11]
head(sports)

# CLUSTERING ###############################################

# Create distance matrix
d <- dist(st)

# Hierarchical clustering
c <- hclust(d)
c # Info on clustering

# Plot dendrogram of clusters
plot(c, main = "Cluster with All Searches and Personality")

# Or nest commands in one line (for sports data)
plot(hclust(dist(sports)), main = "Sports Searches")

```

### EXE 2/2 hierarchical  

```{r}
# Conducting a cluster analysis

# Load data
#?mtcars
data(mtcars)
mtcars[1:5, ]
mtcars1 <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables
mtcars1[1:5, ]

# -------- We'll use hierarchical clustering
# Need distance matrix (dissimilarity matrix)
d <- dist(mtcars1)
head(d)  # Huge matrix

# Use distance matrix for clustering
c <- hclust(d)
c

# Plot dendrogram of clusters
plot(c)

# Put observations in groups
# Need to specify either k = groups or h = height
g3 <- cutree(c, k = 3)  # "g3" = "groups 3"
# cutree(hcmt, h = 230) will give same result
g3

# Or do several levels of groups at once
# "gm" = "groups/multiple"
gm <- cutree(c, k = 2:5) # or k = c(2, 4)
gm

# Draw boxes around clusters
rect.hclust(c, k = 2, border = "gray")
rect.hclust(c, k = 3, border = "blue")
rect.hclust(c, k = 4, border = "green4")
rect.hclust(c, k = 5, border = "darkred")

```


### EXE 1/2 k-means Clustering	
+ by **distance from centroid**  (only convex clusters similar size, you pick k)

`k-means` is on well known _unsupervised_ clustering algorithm that finds the minimum total distance of obs. form the centroid of the (best) cluster (for all dimensions)

(*) _hyperparameter_ `k` = the number of clusters and has to be set beforehand. k should be picked as a # that is in the bal park of what you are willing to act on (eg. 5 if I want 5 segments of donors)

(*) Where (through an iterative approach) it finds:
	+ The distance between data points within clusters should be as small as possible.
   + The distance of the centroids (= centres of the clusters) should be as big as possible.


```{r kmmen}
# ======================================================
mtcars1 <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables

# k-means clustering - I pick 3 clusters 
km <- stats::kmeans(mtcars1, centers=3)
km

# Graph based on k-means
require(cluster)
clusplot(mtcars1,  # data frame
         km$cluster,  # cluster data
         color = TRUE,  # color
#          shade = TRUE,  # Lines in clusters
         lines = 3,  # Lines connecting centroids
         labels = 2)  # Labels clusters and cases

# here I get the centers
km$centers

# here I get the cluster assignment (vector)
km$cluster

# You can evaluate the clusters by looking at $totss and $betweenss.
km$totss
km$withinss # sum of the square of the distance from each data point to the cluster center.  Lower is better. (high withinss would indicate either outliers are in your data or you need to create more clusters.)
km$betweenss # sum of the squared distance between cluster centers.  Ideally you want cluster centers far apart from each other.
 

# ======================================================
# k-means clustering - I pick 5 clusters 
km2 <- kmeans(mtcars1, 5)
km2

# Graph based on k-means
require(cluster)
clusplot(mtcars1,  # data frame
         km2$cluster,  # cluster data
         color = TRUE,  # color
#          shade = TRUE,  # Lines in clusters
         lines = 3,  # Lines connecting centroids
         labels = 2)  # Labels clusters and cases

 # here I get the centers
km2$centers

# You can evaluate the clusters by looking at $totss and $betweenss.
km2$totss
km2$withinss # sum of the square of the distance from each data point to the cluster center.  Lower is better. (high withinss would indicate either outliers are in your data or you need to create more clusters.)
km2$betweenss # sum of the squared distance between cluster centers.  Ideally you want cluster centers far apart from each other.


# ======================================================
# Find ideal k
rng<-2:20 #K from 2 to 20
tries <-100 #Run the K Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points

for(v in rng){ # For each value of the range variable
 v.totw.ss <-integer(tries) #Set up an empty vector to hold the 100 tries
 for(i in 1:tries){
 k.temp <-kmeans(mtcars1,centers=v) #Run kmeans
 v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
 }
 avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}

plot(rng,avg.totw.ss,type="b", main="Total Within SS by Various K",
 ylab="Average Total Within Sum of Squares",
 xlab="Value of K")

# Somewhere around K = 5 we start losing dramatic gains.  So I’m satisfied with 5 clusters.
```




### EXE 2/2 k-means CLUST - THEORY ILLUSTRATION 
Building the algorithm 
http://www.learnbymarketing.com/tutorials/k-means-clustering-in-r-example/
http://blog.ephorie.de/learning-data-science-understanding-and-using-k-means-clustering

Because there are too many possible combinations of all possible clusters comprising all possible data points k-means follows an iterative approach called **_expectation-maximization algorithm_**.:
    1. Initialization: assign clusters randomly to all data points
    2. E-step (for expectation): assign each observation to the “nearest” (based on Euclidean distance) cluster
    3. M-step (for maximization): determine new centroids based on the mean of assigned objects
    4. Repeat steps 3 and 4 until no further changes occur

```{r kmmenSTEP}
n <- 3 # no. of centroids
set.seed(1415) # set seed for reproducibility
 
M1 <- matrix(round(runif(100, 1, 5), 1), ncol = 2)
M2 <- matrix(round(runif(100, 7, 12), 1), ncol = 2)
M3 <- matrix(round(runif(100, 20, 25), 1), ncol = 2)
M <- rbind(M1, M2, M3)
 
C <- M[1:n, ] # define centroids as first n objects
obs <- length(M) / 2
A <- sample(1:n, obs, replace = TRUE) # assign objects to centroids at random
colors <- seq(10, 200, 25) 

# helper function for plotting the steps 
clusterplot <- function(M, C, txt) {
  plot(M, main = txt, xlab = "", ylab = "")
  for(i in 1:n) {
    points(C[i, , drop = FALSE], pch = 23, lwd = 3, col = colors[i])
    points(M[A == i, , drop = FALSE], col = colors[i])    
  }
}
clusterplot(M, C, "Initialization")

#------------k-means algorithm iterative
# diamonds are the Centroids

repeat {
  # calculate Euclidean distance between objects and centroids
  D <- matrix(data = NA, nrow = n, ncol = obs)
  for(i in 1:n) {
    for(j in 1:obs) {
      D[i, j] <- sqrt((M[j, 1] - C[i, 1])^2 + (M[j, 2] - C[i, 2])^2)
    }
  }
  O <- A
   
  ## E-step: parameters are fixed, distributions are optimized
  A <- max.col(t(-D)) # assign objects to centroids
  if(all(O == A)) break # if no change stop
  clusterplot(M, C, "E-step")
   
  ## M-step: distributions are fixed, parameters are optimized
  # determine new centroids based on mean of assigned objects
  for(i in 1:n) {
    C[i, ] <- apply(M[A == i, , drop = FALSE], 2, mean)
  }
  clusterplot(M, C, "M-step")
}

#Check results 
(custom <- C[order(C[ , 1]), ])
##        [,1]   [,2]
## [1,]  3.008  2.740
## [2,]  9.518  9.326
## [3,] 22.754 22.396


#------------k-means algorithm BASE R
cl <- kmeans(M, n)
clusterplot(M, cl$centers, "Base R")

#Check results 
(base <- cl$centers[order(cl$centers[ , 1]), ])
##     [,1]   [,2]
## 2  3.008  2.740
## 1  9.518  9.326
## 3 22.754 22.396

# same!
```




*** 








### RESOURCES 
+ http://www.learnbymarketing.com/tutorials/
+ https://www.linkedin.com/learning/r-statistics-essential-training/computing-a-bivariate-regression (Linkedin Premium)
+ https://www.linkedin.com/learning/data-science-foundations-data-mining/anomaly-detection-in-r (Linkedin Premium)

+ IADB ML course 
+ Hastie, Tibshirani, Firedman "The elements of Statistical Learning", 2009



