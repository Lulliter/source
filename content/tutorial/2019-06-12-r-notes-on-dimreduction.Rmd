---
title: "R Notes on Dimensionality Reduction"
author: "Luisa M. Mimmi"
date: "Last run: `r format(Sys.time(), '%B %e, %Y')`"
draft: true
output:
  blogdown::html_page:
    toc: true
    #toc_float: true # NOT AVAIL FOR HUGO ACADEMIC  https://github.com/rstudio/blogdown/issues/58
    #smart: true
image:
  caption: ''
  focal_point: ''
slug: r-notes-on-dimreduction
tags:
  - dimensionality reduction
  - PCA
  - association
  - outliers
  - sequential patterns
categories:
  - Rtutorial
summary: "Notes on Dimensionality Reduction - UNFINISHED" 
---


> From 
https://www.linkedin.com/learning/data-science-foundations-data-mining/anomaly-detection-in-r
(Linkedin Premium)

> Add 
IADB ML course 


## Set up 
```{r message=FALSE, warning=FALSE}
# # I am  executing bc it for the blog post 
# knitr::opts_chunk$set(eval = TRUE, 
# 							 echo = TRUE, 
# 							 tidy = FALSE, 
# 							 results='hide',  
# 							 message = FALSE, 
# 							 warning = FALSE , fig.show='asis', fig.align='center', 
# 							 fig.width=6, fig.height=6)

 
if (!require("PerformanceAnalytics")) install.packages("PerformanceAnalytics")
if (!require("ggcorrplot")) install.packages("ggcorrplot")
if (!require("GGally")) install.packages("GGally") # Ext to ggplot2 
if (!require("ggpubr")) install.packages("ggpubr")
if (!require("kableExtra")) install.packages("kableExtra")
if (!require("pander")) install.packages("pander") 
```



*** 

# DIMENSIONALITY REDUCTION 

DEFINITION:


EXMAPLES:

ALGORITHMs: 
+ Principal Component Analysis (PCA)
+ Linear Discriminant Analysis (LDA)
+ Generalized Discriminant Analysis (GDA)

***

# PRINCIPAL COMPONENT ANALISYS

From "psych" package documentation (p. 213) _"The primary empirical difference between a components  versus a factor model is the treatment of the variances for each item. Philosophically, components are weighted composites of observed variables while in the factor model, variables are weighted composites of the factors."_


```{r}
# Conducting a principal components/factor analysis

# Load data 
data(mtcars)
mtcars[1:5, ]
mtcars1 <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables
mtcars1[1:5, ]

# ========= Principle components model using default method
# If using entire data frame:
pc <- prcomp(mtcars1,
             center = TRUE,  # Centers means to 0 (optional)
             scale = TRUE)  # Sets unit variance (helpful)

# Or specify variables:
# pc <- prcomp(~ mpg + cyl + disp + hp + wt + qsec + am + 
#                gear + carb, data = mtcars, scale = TRUE)

# ?prcomp  # Generally preferred
# ?princomp  # Very slightly different method, similar to S

# Get summary stats
summary(pc)

# Screeplot
plot(pc)

# Get standard deviations and how variables load on PCs
pc

# See how cases load on PCs
predict(pc)

# Biplot
biplot(pc)

# =========== Factor Analysis
# Varimax rotation by default
# Gives chi square test that number of factors
# is sufficient to match data (want p > .05).
# Also gives uniqueness values for variables,
# variable loadings on factors, and variance
# statistics.
factanal(mtcars1, 1)
factanal(mtcars1, 2)
factanal(mtcars1, 3)
factanal(mtcars1, 4)  # First w/p > .05
```



***

> somewhat related

## (DETECT OUTLIERS) 


```{r}
# DM_05_03.R

# INSTALL AND LOAD PACKAGES ################################

pacman::p_load(ggplot2, grid, gridExtra, robustbase) 

# DATA #####################################################

# Import the data
data = read.csv(here::here("content", "tutorial","data","AnomalyData.csv"))

# Structure of the data
str(data)

# Transform variables to factors
data$PsychRegions = as.factor(data$PsychRegions)
data$region = as.factor(data$region)
data$division = as.factor(data$division)

# UNIVARIATE OUTLIERS ######################################

# Using boxplots for each variable separately

# data.science
u01 <- qplot(data = data, y = data.science, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab=NULL, ylab = NULL, 
         main="data.science") +
       geom_text(aes(label = ifelse(data.science %in% 
         boxplot.stats(data.science)$out,
         as.character(state_code), "")), hjust = 1.5)
u01

# cluster.analysis
u02 <- qplot(data = data,y = cluster.analysis, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL,
         main = "cluster.analysis") +
       geom_text(aes(label = ifelse(cluster.analysis %in% 
         boxplot.stats(cluster.analysis)$out,
         as.character(state_code), "")), hjust = 1.5)
u02

# college
u03 <- qplot(data = data, y = college, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL, 
         main="college") +
       geom_text(aes(label = ifelse(college %in% 
         boxplot.stats(college)$out,
         as.character(state_code), "")), hjust = 1.5)
u03

# startup
u04 <- qplot(data = data, y = startup, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL, 
         main="startup") +
       geom_text(aes(label = ifelse(startup %in% 
         boxplot.stats(startup)$out,
         as.character(state_code), "")), hjust = 1.5)
u04

# entrepreneur
u05 <- qplot(data = data, y = entrepreneur, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL, 
         main="entrepreneur") +
       geom_text(aes(label = ifelse(entrepreneur %in% 
         boxplot.stats(entrepreneur)$out,
         as.character(state_code), "")), hjust = 1.5)
u05

# ceo
u06 <- qplot(data = data, y = ceo, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL, 
         main="ceo") +
       geom_text(aes(label = ifelse(ceo %in% 
         boxplot.stats(ceo)$out,
         as.character(state_code), "")), hjust = 1.5)
u06

# mortgage
u07 <- qplot(data = data, y = mortgage, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL, 
         main="mortgage") +
       geom_text(aes(label = ifelse(mortgage %in% 
         boxplot.stats(mortgage)$out,
         as.character(state_code), "")), hjust = 1.5)
u07

# nba
u08 <- qplot(data = data, y = nba, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL, 
         main="nba") +
       geom_text(aes(label = ifelse(nba %in% 
         boxplot.stats(nba)$out,
         as.character(state_code), "")), hjust = 1.5)
u08

# royal.family
u09 <- qplot(data = data, y = royal.family, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL, 
         main="royal.family") +
       geom_text(aes(label = ifelse(royal.family %in% 
         boxplot.stats(royal.family)$out,
         as.character(state_code), "")), hjust = 1.5)
u09

# Neuroticism
u10 <- qplot(data = data, y = Neuroticism, x = 1, 
         geom = "boxplot", outlier.colour = "#E38942",
         xlim = c(0, 2), xlab = NULL, ylab = NULL, 
         main="Neuroticism") +
       geom_text(aes(label = ifelse(Neuroticism %in% 
         boxplot.stats(Neuroticism)$out,
         as.character(state_code), "")), hjust = 1.5)
u10

# Plot all 10 together
grid.arrange(u01, u02, u03, u04, u05,
             u06, u07, u08, u09, u10,
             nrow = 2, 
             top = "Boxplots: Univariate outliers")

# BIVARIATE OUTLIERS #######################################

# data.science vs cluster.analysis
b1 <- qplot(data = data, 
        x = data.science,
        y = cluster.analysis,
        main = "data.science vs cluster.analysis") +
      stat_ellipse(level = .99, color = "#E38942") +
      geom_text(aes(label =
      ifelse((data.science>1.8 | cluster.analysis>1.6),
        as.character(state_code), "")), 
        hjust = 1.5)
b1 

# mortgage vs ceo
b2 <- qplot(data = data,
        x = mortgage,
        y = ceo, 
        main = "mortgage vs ceo") +
      stat_ellipse(level = .99, color = "#E38942") +
      geom_text(aes(label =
        ifelse(ceo > 2,
        as.character(state_code), "")), 
        hjust = 1.5)
b2

# modern.dance vs Openness
b3 <- qplot(data = data,
        x = modern.dance, 
        y = Openness,
        main = "modern.dance vs Openness") +
      stat_ellipse(level = .99,color = "#E38942") +
      geom_text(aes(label =
        ifelse((modern.dance > 2 | Openness < 30),
        as.character(state_code),"")), 
        hjust = 1.5)
b3

# fifa vs nba
b4 <- qplot(data = data,
        x = fifa,
        y = nba, 
        main = "fifa vs nba") +
      stat_ellipse(level = .99, color = "#E38942") +
      geom_text(aes(label =
        ifelse(fifa > 2,
        as.character(state_code), "")), 
        hjust = 1.5)
b4

# subaru vs escalade
b5 <- qplot(data = data,
        x = subaru,
        y = escalade,
        main = "subaru vs escalade") +
      stat_ellipse(level = .99, color = "#E38942") +
      geom_text(aes(label =
        ifelse(subaru > 2.5,
        as.character(state_code), "")), 
        hjust = 1.5)
b5

# unicorn vs obsfucation
b6 <- qplot(data = data,
        x = unicorn,
        y = obfuscation,
        main = "unicorn vs obfuscation") +
      stat_ellipse(level = .99, color = "#E38942") +
      geom_text(aes(label =
        ifelse((unicorn > 2 | obfuscation > 2),
        as.character(state_code), "")), 
        hjust = 1.5)
b6

# Conscientiousness vs Extraversion
b7 <- qplot(data = data,
        x = Conscientiousness,
        y = Extraversion,
        main = "Conscientiousness vs Extraversion") +
      stat_ellipse(level = .99, color = "#E38942") 
b7

# college vs royal.family
b8 <- qplot(data = data,
        x = college,
        y = royal.family,
        main = "college vs royal.family") + 
     stat_ellipse(level = .99, color = "#E38942") 
b8

# Plot all 8 together
grid.arrange(b1, b2, b3, b4, b5, b6, b7, b8,
  nrow = 2, top = "Bivariate outliers")

# MULTIVARIATE OUTLIERS ####################################

# Measure overall distance of case from other using both
# Mahalanobis distance and a robust measures of distance.

# Create dataset with only quantitative variables
mcd = covMcd(data[-c(1, 2, 28, 29, 30)])

par(mfrow = c(1, 2))
# Mahalanobis vs. robust distance
plot(mcd, 
     which = "dd", 
     labels.id = as.character(data$state_code))
# QQ plot for robust distance
plot(mcd,
     which = "qqchi2",
     labels.id = as.character(data$state_code))


```




## (ASSOCIATION ANALISYS)

```{r}
# DM_06_03.R

# INSTALL AND LOAD PACKAGES ################################

pacman::p_load(arules, arulesViz) 

# DATA #####################################################

## Read transactional data from arules package
data("Groceries")   # Load data
#?Groceries          # Help on data
str(Groceries)      # Structure of data
summary(Groceries)  # Includes 5 most frequent items

# RULES ####################################################

# Set minimum support (minSup) to .001
# Set minimum confidence (minConf) to .75

rules <- apriori(Groceries, 
           parameter = list(supp = 0.001, conf = 0.75))

options(digits=2)
inspect(rules[1:10])

# PLOTS ####################################################

# Scatterplot of support x confidence (colored by lift)
plot(rules)

# Graph of top 20 rules
plot(rules[1:20], 
  method = "graph", 
  control = list(type = "items"))

# Parallel coordinates plot of top 20 rules
plot(rules[1:20], 
  method = "paracoord", 
  control = list(reorder = TRUE))

# Matrix plot of antecedents and consequents
plot(rules[1:20], 
     method = "matrix", 
      control = list(reorder = "none")# ,
	  # measure=c("lift", "confidence")
	  )

# Grouped matrix plot of antecedents and consequents
plot(rules[1:20], method = "grouped")


```





## (SEQUENTIAL PATTERNS (sub-type of association))

Sequence mining --> look for chains in the data (if this happens, that is likely to happen)

+ temporal association
+ ordinal association

EXE: 
- genetic sequencing
- recommendation engines
- marketing offers
- behavioral state switching 

ALGORITHMS
+ Generalized Sequential Patterns
+ Hidden MArkov model 
+ SPADE, Sequential Pattern Discovery using Equivalence classes. 


### EXE Hidden MArkov model 
Hidden Markov models are looking for switches in state conditions, or you might want to say qualitatively distinct patterns of behavior
```{r HMM , eval=FALSE, include=FALSE}
# DM_08_03.R

# INSTALL AND LOAD PACKAGES ################################

pacman::p_load(pacman, depmixS4) # can't downl

# LOAD & EXAMINE DATA ######################################

# We'll use the sample dataset "speed" from depmixS4
data(speed)
str(speed)

# Plot the data
plot(ts(speed[, 1:3]), main = "speed data")

# MODEL DATA ###############################################

# Compare models with different numbers of hidden states.

# Model 1: Joint Gaussian-binomial response with 1 state 
model1 <- depmix(list(rt ~ 1, corr ~ 1), 
            data = speed, 
            nstates = 1,
            family = list(gaussian(), 
              multinomial("identity")))
fm1 <- fit(model1, verbose = FALSE)

# Model 2: HMM with 2 states and Pacc as covariate
model2 <- depmix(list(rt ~ 1, corr ~ 1), 
            data = speed, 
            nstates = 2,
            family = list(gaussian(), 
              multinomial("identity")), 
            transition = ~ scale(Pacc),
              ntimes=c(168, 134, 137))
fm2 <- fit(model2, verbose = FALSE)

# Model 3: HMM with 3 states and Pacc as covariate 
model3 <- depmix(list(rt ~ 1,corr ~ 1), 
            data = speed, 
            nstates = 3,
            family = list(gaussian(), 
              multinomial("identity")), 
            transition = ~ scale(Pacc),
              ntimes=c(168, 134, 137))
fm3 <- fit(model3, verbose = FALSE)

# COMPARE MODELS ###########################################

# Want lowest BIC (Bayesian Information Criterion)
plot(1:3, c(BIC(fm1), BIC(fm2), BIC(fm3)),
  ty = "b", xlab = "Model", ylab = "BIC")

```





```{r clean, eval = FALSE, echo=TRUE}
# # CLEAN UP #################################################
# 
# # Clear workspace
# rm(list = ls()) 
# 
# # Clear packages
# pacman::p_unload(ggplot2, grid, gridExtra, robustbase)
# 
# # Clear plots
# dev.off()  # But only if there IS a plot
# 
# # Clear console
# cat("\014")  # ctrl+L
# 

```

### RESOURCES 
+ http://www.learnbymarketing.com/tutorials/
+ https://www.linkedin.com/learning/r-statistics-essential-training/computing-a-bivariate-regression (Linkedin Premium)




