---
title: "R Notes on Correlation, Association Measures, etc."
author: "Luisa M. Mimmi"
date: "Last run: `r format(Sys.time(), '%B %e, %Y')`"
draft: false
output:
  blogdown::html_page:
    toc: true
    # toc_float: true error in this theme https://github.com/rstudio/blogdown/issues/58
    #smart: true
image:
  caption: ''
  focal_point: ''
slug: r-notes-on-correlation
tags:
  - corr
  - t-test
  - ANOVA
categories:
  - Rtutorial
summary: "Recap of correlation between different types of variables - UNFINISHED" 
---

# CORRELATION AND HOW TO MEASURE IT DEPENDING ON TYPE OF VARIABLE(S)


### Set UP 
```{r message=FALSE, warning=FALSE}
# I am  executing bc it for the blog post 
knitr::opts_chunk$set(eval = TRUE, 
							 echo = TRUE, 
							 tidy = FALSE, 
							 results='hide',  
							 message = FALSE, 
							 warning = FALSE , fig.show='asis', fig.align='center', 
							 fig.width=6, fig.height=6)

# POV is where .toml is? 
load(here::here("content","tutorial", "AC_ppl_fakeID.Rdata"), verbose = T)

if (!require("PerformanceAnalytics")) install.packages("PerformanceAnalytics")
if (!require("ggcorrplot")) install.packages("ggcorrplot")
if (!require("GGally")) install.packages("GGally") # Ext to ggplot2 
if (!require("ggpubr")) install.packages("ggpubr")
if (!require("kableExtra")) install.packages("kableExtra")
if (!require("pander")) install.packages("pander") 
```


 
## Between 2 CONTINUOUS variables

### Pearson correlation (r)

Pearson correlation (r) measures a linear dependence between 2 CONTINUOUS variables (x and y) / or 2 dichotomous var 
> It’s also known as a parametric correlation test because it depends to the distribution of the data. 
> The Pearson correlation evaluates the linear relationship between two continuous variables. 


 
#### FORMULA
In the formula below,

= $x$ and $y$ are two vectors of length $n$
= $m_x$ and $m_y$ corresponds to the means of x and y, respectively.

$$ r = \frac{\sum{(x-m_x)(y-m_y)}}{\sqrt{\sum(x-m_x)^2\sum(y-m_y)^2}} $$

The p-value (significance level) of the correlation can be determined :

by calculating the t value as follow:

$$ t={\frac{r}{\sqrt{(1−r^2)}}}\sqrt{n−2}$$ 

In the case 2) the corresponding p-value is determined using t distribution table for df=n−2

```{r}
# For example 

# x = "Acc_Don_NYE2019__c"  
# y = "Acc_Don_NYE2011__c"
cor(AC_ppl_fakeID$Acc_Don_NYE2011__c, AC_ppl_fakeID$Acc_Don_NYE2019__c, 
	 method = c("pearson" )) # 0.17
cor(AC_ppl_fakeID$Acc_Don_NYE2018__c, AC_ppl_fakeID$Acc_Don_NYE2019__c, 
	 method = c("pearson" )) # 0.68

# Pearson correlation test 
res11 <- cor.test(AC_ppl_fakeID$Acc_Don_NYE2011__c, AC_ppl_fakeID$Acc_Don_NYE2019__c, 
						method = "pearson")
res11

res18 <- cor.test(AC_ppl_fakeID$Acc_Don_NYE2018__c, AC_ppl_fakeID$Acc_Don_NYE2019__c, 
						method = "pearson")
res18

# Visualize your data using scatter plots
library("ggpubr")
ggscatter(AC_ppl_fakeID, x = "Acc_Don_NYE2011__c", y = "Acc_Don_NYE2019__c", 
			 add = "reg.line", conf.int = TRUE, 
			 cor.coef = TRUE, cor.method = "pearson",
			 xlab = "Acc_Don_NYE2011__c", ylab = "Acc_Don_NYE2019__c")

library("ggpubr")
ggscatter(AC_ppl_fakeID, x = "Acc_Don_NYE2018__c", y = "Acc_Don_NYE2019__c", 
			 add = "reg.line", conf.int = TRUE, 
			 cor.coef = TRUE, cor.method = "pearson",
			 xlab = "Acc_Don_NYE2018__c", ylab = "Acc_Don_NYE2019__c")
```


#### More Visualization
```{r}
# SELECT A FEW NUMER 
data_numeric <- AC_ppl_fakeID[ , purrr::map_lgl(AC_ppl_fakeID, is.numeric)] 

data_numeric2 <- data_numeric %>% 
	dplyr::filter(Acc_Don_Tot >0) %>% 
	dplyr::select( 4:12)

# === modo A) 
library(PerformanceAnalytics)
cormat <- chart.Correlation(data_numeric2, histogram=TRUE, pch=19)


# === modo B)  
	# 1) Compute a matrix of correlation p-values
	library(ggcorrplot)
	p.mat <- cor_pmat(data_numeric2)
	p.mat 

	# 2) Compute a correlation matrix
corr.mat <- round(cor(data_numeric2, use = "pairwise.complete.obs"), # USE everything", "all.obs", "complete.obs", "na.or.complete"
						digits = 1)
corr.mat
	# 3) Visualize the correlation matrix
ggcorrplot(corr.mat , hc.order = F, type = "lower", method = "square", #method = "circle"
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"),lab = TRUE, p.mat = p.mat)

# === modo C) 
library(GGally)
 GGally::ggcorr(data = data_numeric2 ,
	# geom object to use. Accepts either "tile", "circle", "text" or "blank"
	geom = "tile" , label = TRUE,	label_alpha = T, hjust = 0.9, layout.exp = 2
	)  


```


## Between 2 ORDINAL, INTERVAL or RATIO variables (x and y)

### a) Spearman rank-order correlation 

> The Spearman's rank-order correlation is the nonparametric version of the Pearson product-moment correlation. Spearman's correlation coefficient, (ρ, also signified by rs) measures the strength and direction of association between two ranked variables.

Also called `Spearman's rank-order correlation` 


####  FORMULA
> add from [here](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#what-is-correlation-test)
> ... 



Ordinal (growing class of Donor) vs ordinal (growing most recent email opened)
```{r}
# USING  DON CLASSES AS PER JOSES'
skimr::sorted_count(AC_ppl_fakeID$DonorClass) # ordine crescete (highere more generous)
# AND FACTOR RECENT OPEN EMAIL 
skimr::sorted_count(AC_ppl_fakeID$EmLastMostRecent)# ordine crescete (higher more recent )
table(AC_ppl_fakeID$EmLastMostRecent, AC_ppl_fakeID$DonorClass)

# ============Spearman’s rho statistic 

corr1 <- cor.test(AC_ppl_fakeID$DonorClass, as.numeric(AC_ppl_fakeID$EmLastMostRecent),  method = "spearman" )
corr1

# library("Hmisc")
# corr1b <- rcorr(AC_ppl3$Acc_Don_Tot, AC_ppl3$EmLastMostRecent,type = "spearman" )
# corr1b

library("ggpubr")
ggscatter(AC_ppl_fakeID, x = "EmLastMostRecent", y = "DonorClass", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "How recently opened an email", ylab = "Class Donor")

# Very week negative correlation rho ~ -0.06094809
```
	

###  b) Kendall tau

Kendall tau is another rank-based correlation coefficients (non-parametric)
> The Kendall rank correlation coefficient or Kendall’s tau statistic is used to estimate a rank-based measure of association. This test may be used if the data do not necessarily come from a bivariate normal distribution.

The Kendall correlation method measures the correspondence between the ranking of x and y variables. The total number of possible pairings of x with y observations is n(n−1)/2, where n is the size of x and y.


The procedure is as follow: 
+ Begin by ordering the pairs by the x values. If x and y are correlated, then they would have the same relative rank orders.
+ Now, for each yi, count the number of ${y_{j}}$ > ${y_{i}}$ (**concordant pairs (c)**) and the number of ${y_{j}}$<${y_{i}}$ (**discordant pairs (d)**).

#### FORMULA
Kendall correlation distance is defined as follow:

$$tau=\frac{n_{c}-n_{d}}{\frac{1}{2}n(n-1)}$$

Where 
${n_{c}}$:  total number of concordant pairs
${n_{d}}$:  total number of discordant pairs
${n}$:  size of x and y

> from [kendall](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#what-is-correlation-test)


```{r}
corrKend <- cor.test(AC_ppl_fakeID$Acc_Don_Tot, as.numeric(AC_ppl_fakeID$EmLastMostRecent),  method = "kendall" )
corrKend
# Very week negative correlation tau ~ -0.09864503 

corrKend2 <- cor.test(AC_ppl_fakeID$DonorClass, as.numeric(AC_ppl_fakeID$EmLastMostRecent),  method = "kendall" )
corrKend2
# Very week negative correlation tau ~ -0.05578083 
```


## Between 2 CATEGORICAL variables
When data includes 2 categorical variables, the most common test for that association is going to be the **Chi Squared test for Independence** (sometimes called `Crosstabs`).
 

### Chi Squared test for Crosstabs with categorical variables 
EXE 1 :  from titanic 

(*) Quickly check with  `stats::ftable(..., exclude = c(NA, NaN), row.vars = NULL,col.vars = NULL)`

```{r}
# smaller 
AC_red<- AC_ppl_fakeID %>% 
	dplyr::select(IsDonor, HasEmail)



# Creating crosstabs for categorical variables

# Load data
ftable(AC_red)  # Makes "flat" table

ttab <- table(AC_red$IsDonor, AC_red$HasEmail)
# Call also get cell, row, and column %
# With rounding to get just 2 decimal places
# Multiplied by 100 to make %
round(prop.table(ttab, 1), 2) * 100 # row %
round(prop.table(ttab, 2), 2) * 100 # column %
round(prop.table(ttab), 2) * 100    # cell %

# Chi-squared test
# (Test of association to see if 
# H0: the 2 cat var (Class & Survived) are independent
# H1: the 2 cat var are correlated in __some way__

 
tchi <- chisq.test(ttab)
tchi
# RESULTS  p-value < 0.00000000000000022 --> We reject the null hypothesis that the class is independent of survival
# (see the row % --> the higher the class, the higher the % surv)

# Additional tables (from results)
tchi$observed   # Observed frequencies (same as ttab)
tchi$expected   # Expected frequencies
tchi$residuals  # Pearson's residual
tchi$stdres     # Standardized residual

 
```




### Chi-Squared Tests + Crammer's V for testing relationships between 2  Categorical variables	
EXE 2 : "IsDonor" (binary var) vs. ("HasValidEmail" or "HasValidEmailUnsubscr")

> Theory
# https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab
# https://www.spss-tutorials.com/cramers-v-what-and-why/

> Also known as Cramér’s phi (coefficient) --> $\phi_{c}$  denotes Cramér’s V

1) Checking if two categorical variables are independent  --> This is a typical Chi-Square test: if we assume that two variables are independent, then the values of the contingency table for these variables should be distributed uniformly. 
2) Crammer's V that is a measure of correlation that follows from this test

H0: the 2 cat var are independent
H1: the 2 cat var are correlated in some way 

#### Crammer's V  CORRELATION FORMULA 

$$\phi_{c}=\sqrt{\frac{\chi^2}{N(k-1)}} $$

where:

$\phi_{c}$ denotes Cramér’s V _("phi" coefficient)_
$\chi^2$ is the Pearson chi-square statistic from the aforementioned test;
$N$ is the sample size involved in the test and
$k$ is the lesser number of categories of either variable.


Creating a function and calculating Cramér V / Phi
```{r}
# Two Categorical Variables "IsDonor" VS "HasValidEmail"        OR      "HasValidEmailUnsubscr" 

##======== 1) Chi-Squared test of independence.
ChisqStat <- chisq.test(AC_ppl_fakeID$DonorClass,AC_ppl_fakeID$HasValidEmail , correct=FALSE) 
ChisqStat$statistic[1] # 36.94155 
ChisqStat$p.value[1]  # 1.851939e-07  --> yes independent! 


##==========2) a Crammer's V that is a measure of correlation that follows from this test
# Function
cv.test = function(x,y) {
  ### = Square root of the Pearson chi-square (without the Yates correction) 
	Chi2stat = chisq.test(x, y, correct=FALSE)$statistic
	Chi2p = chisq.test(x, y, correct=FALSE)$p.value
	CV = sqrt(Chi2stat / 
						(length(x) * # is divided by N = the sample size  ...
						(min(length(unique(x)),length(unique(y))) - 1))) # *k = (smallest b/t categories (row or column) -1)
# 	print.noquote("Cramér V / Phi:") # “phi coefficient”
#   return(as.numeric(CV))
  return(list(Chi2stat = as.numeric(Chi2stat), Chi2p = as.numeric(Chi2p), CVphi = round(as.numeric(CV), 4)))
	
}
					# with(top3cit, cv.test(x, y)) # [1] Cramér V / Phi: 0.09052046

## =========3)  call Cramer V function 
# DOnor class & ... 
with(AC_ppl_fakeID, cv.test(DonorClass, ConnectionLevel__c_NA)) #  [1] Cramér V / Phi:0.2168275 !!
with(AC_ppl_fakeID, cv.test(DonorClass, HasValidEmail)) #   [1] Cramér V / Phi:0.0527
```


Calculating (many) of them and rendering in a table for the presentation 
```{r}
ConnectionPhi <- cv.test(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$ConnectionLevel__c_NA) 
# Gender__cPhi <- cv.test2(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$Gender__c) 
VIP_Phi <- cv.test(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$VIP) 
SPeakerPhi <- cv.test(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$Speaker) 
VolPhi <- cv.test(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$Volunteer) 
HasEmailPhi <- cv.test(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$HasEmail) 
HasValidEmaiPhi <- cv.test(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$HasValidEmail) 
HAsEmVAlUnsubsPhi <- cv.test(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$HasValidEmailUnsubscr) 
LASTOPENED <- cv.test(AC_ppl_fakeID$IsDonor, AC_ppl_fakeID$EmLastOpenFactor)


PhiResults <- c(print(ConnectionPhi[[3]]),   print(VIP_Phi[[3]]), print(SPeakerPhi[[3]]), 
					 print(VolPhi[[3]]), print(HasEmailPhi[[3]]), print(HasValidEmaiPhi[[3]]), print(HAsEmVAlUnsubsPhi[[3]]), print(LASTOPENED[[3]]))

CatXs <- c( "Connection",
				# "Gender", 
				"VIP status", 
				"Speaker status",
				"Volunteer status",
				"Has Email", 
				"Has Valid Email", 
				"Has Valid Email but Unsubscribed",
				"How recently Opened Last Email")

CramerVResults <- cbind(PhiResults, CatXs)
colnames(CramerVResults) <- c("Cramer's V phi statistic" , "Categorical Var")

CramerVs <- knitr::kable(CramerVResults, 
								 caption = "Measure of association between \"Is Donor\" and some Categorical Variables") %>%
	kableExtra::kable_styling("striped", full_width = F, latex_options = "scale_down",position = "center") %>%
	row_spec(1 , background = "yellow") %>%
	row_spec(5:6, background = "yellow") %>%
	row_spec(8, background = "yellow") # %>%
# as_image(., width = NULL, height = NULL, file = "./figures/CramerVs.png" ) 

CramerVs
```

Then seeing it graphically with BoxPlots

```{r}
# ======= Connection
# average PER YEAR 
plot1 <- ggplot(AC_ppl_fakeID) +
  aes(x = ConnectionLevel__c_NA) + 
  aes(y = Acc_Don_NYE2019__c) +
  geom_jitter(alpha = .5, height = 0, width = .25) +
  aes(col = ConnectionLevel__c_NA) +
  geom_boxplot(alpha = .25) +
  aes(fill = ConnectionLevel__c_NA) +
	ylim(0,30000) +
ggthemes::theme_hc() + ggthemes::scale_color_pander() +  #scale_color_colorblind() + # scale_color_hc() # scale_colour_ptol()  +
labs(x = "") + labs(y = "Us Dollars") +
theme(legend.position="none") + # remove legent & ... 
labs(title = "Contributions to NYE 2019") +
labs(subtitle = "Stratified by \"Connection level\" of Individual/Households") +
labs(caption = paste0("Created on ", Sys.Date())) +
	coord_flip()  

plot1
# Acc_Don_Tot
plot2 <-ggplot(AC_ppl_fakeID) +
  aes(x = ConnectionLevel__c_NA) + 
  aes(y = Acc_Don_Tot) +
  geom_jitter(alpha = .5, height = 0, width = .25) +
  aes(col = ConnectionLevel__c_NA) +
  geom_boxplot(alpha = .25) +
  aes(fill = ConnectionLevel__c_NA) +
	ylim(0,30000) +
ggthemes::theme_hc() + ggthemes::scale_color_pander() +  #scale_color_colorblind() + # scale_color_hc() # scale_colour_ptol()  +
labs(x = "") + labs(y = "Us Dollars") +
labs(title = "TOTAL Contributions between NYE 2011 - NYE 2019") +
labs(subtitle = "Stratified by \"Connection level\" of Individual/Households") +
labs(caption = paste0("Created on ", Sys.Date())) +
	coord_flip()

plot2
# save the 2 together 
#ggsave("./figures/BoxPLOTConnection_ALL.png", gridExtra::arrangeGrob(plot1, plot2))
```



## Between 1 CONTINUOUS variable AND 1 CATEGORICAL variable

EXE: Donation amount and ConnectionLevel__c_NA

### a) One-Way ANOVA (F-Test)

ANOVA and One-Way ANOVA F-Test for testing relationships between Numerical and Categorical variables
FOLLOWING: http://www.sthda.com/english/wiki/one-way-anova-test-in-r#what-is-one-way-anova-test
CHECK also: https://thomasmock.netlify.com/post/a-gentle-guide-to-tidy-statistics-in-r/
CHECK also: https://ademos.people.uic.edu/Chapter20.html


GOAL: for comparing means in a situation where there are more than two groups

$H_0$ : Null Hyp =  the means of the different groups are the same
$H_1$ : Alternative hyp = At least one sample mean is not equal to the others.

NOTE: 
+ It only looks if **in general** the groups differ (but not specifically by how much and which of them)
+ Better to have data in long form

```{r}
# --------One-Way ANOVA F-Test
# Show the levels
levels(AC_ppl_fakeID$ConnectionLevel__c_NA)

aov1 = aov( Acc_Don_NYE2019__c ~ ConnectionLevel__c_NA, data = AC_ppl_fakeID)
summary(aov1) # diff between the groups significant at 0.001

aov2 = aov(Acc_Don_NYE2019__c ~ HasEmail, data = AC_ppl_fakeID )
summary(aov2) # diff between the groups significant at 0.001

aov3 = aov(Acc_Don_NYE2019__c ~ EmLastOpenFactor, data = AC_ppl_fakeID )
summary(aov3)  # diff between the groupsnot sig 

```


### b) Multiple pairwise-comparison between the groups' means ANOVA  
**Tukey multiple pairwise-comparisons** for post-hoc comparison 

GOAL: As the ANOVA test is significant, we can compute Tukey HSD (Tukey Honest Significant Differences, R function: TukeyHSD()) for performing multiple pairwise-comparison between the means of groups.

The function TukeyHD() takes the fitted ANOVA as an argument.

INTERPRETATION: 
**diff**: difference between means of the two groups
**lwr, upr**: the lower and the upper end point of the confidence interval at 95% (default)
**p adj**: p-value after adjustment for the multiple comparisons.

```{r}
# ==== Group by group across ConnectionLevel__c_NA
# aov1 = aov( Acc_Don_NYE2019__c ~ ConnectionLevel__c_NA, data = AC_ppl_fakeID)
TukeyRes <- stats::TukeyHSD(aov1, ordered = F)  

# ==== Show table
# OR 
# TukeyResTukeyRes[1:1] # 
TukeyRes_df <- as.data.frame(TukeyRes$ConnectionLevel__c_NA) 

# # ==== Display 
caption <- "Pairwise-comparison between the means of groups (ConnectionLevel Types)"
TukeyRes_df_table <- pandoc.table.return(TukeyRes_df, 
													  keep.line.breaks = TRUE, 
													  style = "simple", 
													  justify = "lrrrr", 
													  caption = caption)
cat(TukeyRes_df_table)


```


### a) Two-Factor ANOVA (F-Test)

**Show also an interaction.** The beauty of a two factor model is that you can see, not only how factor A affects the mean and factor B affects the mean but the interplay of A and B, specifically to see if the effect of one factor is moderated by the levels of another factor. 


```{r}
# Comparing means with a two-factor ANOVA

# Get an Idea... 
boxplot(Acc_Don_Tot ~ IsDonor*Volunteer, data = AC_ppl_fakeID)


# Model with interaction
aov1 <- aov(Acc_Don_Tot ~ 
            # IsDonor + Volunteer + IsDonor:Volunteer, 
            IsDonor*Volunteer,
				data = AC_ppl_fakeID)
summary(aov1)

# Additional information on model
model.tables(aov1)
model.tables(aov1, type = "means")
model.tables(aov1, type = "effects")  # "effects" is default

# Post-hoc test
TukeyHSD(aov1)

```

*** 
# COMPARING MEANS 

## T-test to compare means of independent groups 

```{r}
# Comparing means with the t-test

# Load data
# ?sleep
sleep[1:5, ]
sd <- sleep[, 1:2]  # Save just the first two variables
sd[1:5, ]  # Show the first 5 cases

# Some quick plots to check data
hist(sd$extra, col = "lightgray")
boxplot(extra ~ group, data = sd)

# Independent 2-group t-test (with defaults)
t.test(extra ~ group, data = sd)

# t-test with options
t.test(extra ~ group,
       data = sd,
       alternative = "less",  # One-tailed test
       conf.level = 0.80)  # 80% CI (vs. 95%)

# Create two groups of random data in separate variables
# Good because actual difference is known
x <- rnorm(30, mean = 20, sd = 5)
y <- rnorm(30, mean = 22, sd = 5)
t.test(x, y)

# rm(list = ls())  # Clean up
```

## Paired T-test to compare means of paired groups (e.g. same group in t1 and t2)

```{r}

# Load data
# Create random data
t1 <- rnorm(50, mean = 52, sd = 6)  # Time 1
dif <- rnorm(50, mean = 6, sd = 12)  # Difference
t2 <- t1 + dif  # Time 2

# Some quick plots to check data
hist(t1)
hist(dif)
hist(t2)
boxplot(t1, t2)

# Save variables in dataframe and use "MASS"
# to create parallel coordinate plot
pairs <- data.frame(t1, t2)
require("MASS")
parcoord(pairs, var.label = TRUE)

# Paired t-test (with defaults)
t.test(t2, t1, paired = TRUE)

# Paired t-test with options
t.test(t2, t1, 
       paired = TRUE,
       mu = 6,  # Specify non-0 null value
       alternative = "greater",  # One-tailed test
       conf.level = 0.99)  # 99% CI (vs. 95%)

# Clean up
detach("package:MASS", unload=TRUE)
# rm(list = ls())

```

# COMPARING PROPORTIONS 

(*) % of success across groups 
```{r}
# Comparing proportions

# Load data
# Need two vectors:
# One specifies the total number of people in each group
# This creates a vector with 5 100s in it, for 5 groups
# Same as "number of trials"
n5 <- c(rep(100, 5))
# Another specifies the number of people who are in category
# Same as "number of successes"
x5 <- c(65, 60, 60, 50, 45)
prop.test(x5, n5)

# If there are only two groups, then it gives a confidence
# interval for the difference between the groups; 
# the default CI is .95
n2 <- c(40, 40)  # Number of trials
x2 <- c(30, 20)  # Number of successes
prop.test(x2, n2, conf.level = .80)

 
```


*** 

# CLUSTER ANALISYS
```{r}
# Conducting a cluster analysis

# Load data
#?mtcars
data(mtcars)
mtcars[1:5, ]
mtcars1 <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables
mtcars1[1:5, ]

# Three major kinds of clustering:
#   1. Split into set number of clusters (e.g., kmeans)
#   2. Hierarchical: Start separate and combine
#   3. Dividing: Start with a single group and split

# We'll use hierarchical clustering
# Need distance matrix (dissimilarity matrix)
d <- dist(mtcars1)
d  # Huge matrix

# Use distance matrix for clustering
c <- hclust(d)
c

# Plot dendrogram of clusters
plot(c)

# Put observations in groups
# Need to specify either k = groups or h = height
g3 <- cutree(c, k = 3)  # "g3" = "groups 3"
# cutree(hcmt, h = 230) will give same result
g3
# Or do several levels of groups at once
# "gm" = "groups/multiple"
gm <- cutree(c, k = 2:5) # or k = c(2, 4)
gm

# Draw boxes around clusters
rect.hclust(c, k = 2, border = "gray")
rect.hclust(c, k = 3, border = "blue")
rect.hclust(c, k = 4, border = "green4")
rect.hclust(c, k = 5, border = "darkred")

# k-means clustering
km <- kmeans(mtcars1, 3)
km

# Graph based on k-means
require(cluster)
clusplot(mtcars1,  # data frame
         km$cluster,  # cluster data
         color = TRUE,  # color
#          shade = TRUE,  # Lines in clusters
         lines = 3,  # Lines connecting centroids
         labels = 2)  # Labels clusters and cases

 
```

# PRINCIPAL COMPONENT ANALISYS

From "psych" package documentation (p. 213) _"The primary empirical difference between a components  versus a factor model is the treatment of the variances for each item. Philosophically, components are weighted composites of observed variables while in the factor model, variables are weighted composites of the factors."_


```{r}
# Conducting a principal components/factor analysis

# Load data 
# ?mtcars
data(mtcars)
mtcars[1:5, ]
mtcars1 <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables
mtcars1[1:5, ]

# ========= Principle components model using default method
# If using entire data frame:
pc <- prcomp(mtcars1,
             center = TRUE,  # Centers means to 0 (optional)
             scale = TRUE)  # Sets unit variance (helpful)

# Or specify variables:
# pc <- prcomp(~ mpg + cyl + disp + hp + wt + qsec + am + 
#                gear + carb, data = mtcars, scale = TRUE)

# ?prcomp  # Generally preferred
# ?princomp  # Very slightly different method, similar to S

# Get summary stats
summary(pc)

# Screeplot
plot(pc)

# Get standard deviations and how variables load on PCs
pc

# See how cases load on PCs
predict(pc)

# Biplot
biplot(pc)

# =========== Factor Analysis
# Varimax rotation by default
# Gives chi square test that number of factors
# is sufficient to match data (want p > .05).
# Also gives uniqueness values for variables,
# variable loadings on factors, and variance
# statistics.
factanal(mtcars1, 1)
factanal(mtcars1, 2)
factanal(mtcars1, 3)
factanal(mtcars1, 4)  # First w/p > .05
```




### RESOURCES 
+ http://www.sthda.com/english/wiki/one-way-anova-test-in-r#what-is-one-way-anova-test
+ http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r 
+ http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software
+ https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggally/ggcorr/
+ https://www.linkedin.com/learning/r-statistics-essential-training/computing-a-bivariate-regression (Linkedin Premium)
+ https://thomasmock.netlify.com/post/a-gentle-guide-to-tidy-statistics-in-r/







